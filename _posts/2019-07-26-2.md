---
title: Paper #1: Generative Adversarial Nets - lan Goodfellow et al. 2014
author: M.Y
layout: post
---



# 내 맘대로 읽는 논문 #1

### Generative Adversarial Nets - lan Goodfellow et al. 2014

https://arxiv.org/abs/1406.2661

위 글은 위키피디아 및 Jaejun Yoo's Playground, ratsgo's blog등을 참고하여 작성하였습니다.

```
50% |#################                                                    |
```



- 생성 모델은 Maximum likelihood 를 구하는 것이라 할 수 있다.

  [위키피디아 Maximum Likelihood]

 ![1564152214181](https://user-images.githubusercontent.com/48951818/62408860-09b07c00-b60a-11e9-9bdf-e65cde34b6fa.png)





​			[모수와 표본]

![1564151808945](https://user-images.githubusercontent.com/48951818/62408807-18e2fa00-b609-11e9-80af-45e5a54caec1.png)

[확률변수 X , 확률 밀도 함수 f(x) ]

확률분포는 이산확률분포와 연속확률분포가 있다.

**확률밀도함수** f 는 연속확률분포를 표현하는  함수이다 .  

p(a<=X<=b)=![1564152999611](https://user-images.githubusercontent.com/48951818/62408863-1d5be280-b60a-11e9-985f-1a49be0e6484.png)


위 식은 확률 변수의 값이 [a,b] 일 확률에 대한 식이다. 

예를 들어,  어떤 초등학생 집단의 키에 관한 연속 확률 분포가 다음과 같을때 

**확률변수 X**=키 가 되며, 위 식은 '156cm 이상 163cm이하' 일 확률을 나타낸다. 

즉, **어떤 분포**가 존재하며 이 분포는 **모수θ**에 따를 때, 확률변수 X의 값과 확률 분포함수에 따라 **다른 표본 x가 추출 (Sampling)** 되는 것이다. 예를 들어 위 초등학생 집단의 키에 관한 연속 확률 분포는 정규분포를 따르며  모수가 {평균= 165cm, 분산= 10} 이라고 하자 이때 추출되는 x 는 165cm에 가까운 값일 확률이 높지만 192cm 같은 값 또한 추출된다는 것을 기억하자. 

https://datascienceschool.net/view-notebook/56e7a25aad2a4539b3c31eb3eb787a54/

 

![1564152168748](https://user-images.githubusercontent.com/48951818/62408866-2482f080-b60a-11e9-9e02-9bb70b16b4e1.png)
 

그렇다면 생성모델의 목표는 무엇일까.

- 확률변수가 x(또는 data), g(또는 model) 에 대해 확률분포 Px=Pg 를 만드는 것이고, 다르게 얘기하면 x에대해 가장 likely 한 분포를 만드는 Maximum likelyhood를 이용하는 것이다. 



위 공식을 간단히 나타내면 ***argmax E  Log( Pθ(x) )*** 이며, x 가 우리가 모르는 latent 확률변수 Z 로부터 생성된다고 가정한다면 ( GAN에서는 노이즈 벡터/ encoder 결과 ) 

marginalization에 따라 

![1564156929705](https://user-images.githubusercontent.com/48951818/62408868-2a78d180-b60a-11e9-8fe4-69931d12c510.png)

로 나타 낼 수 있지만, 위는 적분이 불가능하다(intractable), p(x|z)는 모르는 값으로 적분을 할 수 없다.



이 과정에서 Pθ(x)의 tractable한 lower bound를 유도(variational inference)한 것이 **Variational Auto Encode**r 이며, **GAN**의 경우 Pθ(x)를 직접 다루지 않지만  손실함수가 Px=Pg 의 방향으로 수렴함을 보인다. 이는 strict 한 증명은 아니지만 성능이 매우 좋아 많이 쓰인다.

 

------

다시, GAN으로 돌아와 GAN의 손실 함수 및 배경에 대한 간단한 설명이었다.

그외에 논문상 previous works 대한 부분은 후에 추가하겠다.  ~~언제..~~

![1564157668021](https://user-images.githubusercontent.com/48951818/62408880-47ada000-b60a-11e9-8799-7cf1c4f84a7d.png)


논문 내용은 새로운 framework (mlp 모델 2개의 minmax game)과 함께 손실함수가 과연 최적의 분포를 찾으며, 수렴하는 지를 소개하고 있다.

우선 손실함수를 살펴보자 

![1564158020440](https://user-images.githubusercontent.com/48951818/62408886-51cf9e80-b60a-11e9-9a65-db34af05b6b4.png)



- 미리 얘기하자면 손실함수는binary cross- entropy loss 이지만,  KLD  및 JSD (확률 분포간 거리)로 유도 되어 pg=pdata 를 증명한다.  또한 cross- entropy loss는 ML과도 깊은 관련이 있다..

* 처음에 위의 E 가 헷갈렸는데, 이는 데이터 x 가 여러 개이기 때문이다.



먼저,  최적의 경우 

![1564159079527](https://user-images.githubusercontent.com/48951818/62408890-5dbb6080-b60a-11e9-93db-f4231b825dfc.png)



이다. 이는 손실함수의 optima를 찾기 위해 이를 *미분* 해보면 알 수 있다. 이 때 위 손실함수는 

아래 확률변수의 기댓값인 E의 정의에따라 다음과 같이 정리된다.

![1564155656778](https://user-images.githubusercontent.com/48951818/62408891-62801480-b60a-11e9-862f-e75f1c47cf4c.png)
![1564159165164](https://user-images.githubusercontent.com/48951818/62408892-66ac3200-b60a-11e9-9fc7-52a1ae2fd723.png)



즉,최적의 경우 pg=pdata이므로 D(x)=1/2 임을 알 수 있다. 최적의 경우 손실함수를 정리하면 다음과 같이 손실함수는 Px와 Pg의 거리를 최소화 하고 있음을 알 수 있다. 

![1564159614639](https://user-images.githubusercontent.com/48951818/62408894-6b70e600-b60a-11e9-988b-4cd2687a06f6.png)


저자가 제안하는 학습 방법은 D->G->D->G->....로 번갈아 학습(다른 것은 fix )하는 것이다. 또한 초기에 D(G(z))는 0에 매우 가깝기 때문에  log(1- D(G(x)))또한 0에 가까워져 saturate 할 수 있으므로 min log(1- D(G(x))) 대신 max log(D(G(x))로 학습가능하다고 설명하고 있다.



